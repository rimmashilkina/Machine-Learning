{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32e1ce20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_snippets import *\n",
    "import json, os, urllib.request\n",
    "import pandas as pd, numpy as np\n",
    "import os, random\n",
    "import torch\n",
    "import torch, torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from tqdm.notebook import tqdm\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.io import read_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f76af4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total caption rows: 504413\n"
     ]
    }
   ],
   "source": [
    "# load and parse JSONL captions\n",
    "url = \"https://storage.googleapis.com/localized-narratives/annotations/open_images_train_v6_captions.jsonl\"\n",
    "DEST_DIR = os.path.expanduser(\"~/Projects/img_captioning/data\")\n",
    "FILENAME = \"open_images_train_captions.jsonl\"\n",
    "DEST_PATH = os.path.join(DEST_DIR, FILENAME)\n",
    "\n",
    "rows = []\n",
    "with open(DEST_PATH, 'r') as f:\n",
    "    for js in f:\n",
    "        try:\n",
    "            d = json.loads(js.strip())\n",
    "            rows.append({\"image_id\": d[\"image_id\"], \"caption\": d[\"caption\"]})\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "data = pd.DataFrame(rows).drop_duplicates(\"image_id\").reset_index(drop=True)\n",
    "print(\"Total caption rows:\", len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7bbbe75a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions after filtering: 2091\n",
      "Train images: 1672\n",
      "Val images: 419\n",
      "\n",
      "Sample train rows:\n",
      "           image_id                                            caption\n",
      "0  46e58e6c8d11cd70  To this statue there is a cloth and objects. B...\n",
      "1  a0e1fd6810824cd8  In this picture I can see few toilet paper rol...\n",
      "2  06091e5f7e946d3d  In this image there is a camera on a table, in...\n",
      "3  ed23444b40d5aa86  Here we can see a man presenting a award to a ...\n",
      "4  7f6926d1659d3caa  At the bottom of this image, there are plants ...\n",
      "\n",
      "Sample val rows:\n",
      "           image_id                                            caption\n",
      "0  a991a3f532d4d0f9  In front of the image there is a person drinki...\n",
      "1  b3e0c474f1cffdce  In this picture there is a girl in the center ...\n",
      "2  b7d04dcbac05475d  In this image there is a tissue holder and on ...\n",
      "3  efb8555f62112d9a  In the center of the image we can see a man st...\n",
      "4  29c5019f5023618c  This picture is a black and white image. In th...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#filter the caption DataFrame to include only those image IDs that exist in the image folders\n",
    "train_files = {f.replace(\".jpg\", \"\") for f in os.listdir(\"train-images\") if f.endswith(\".jpg\")}\n",
    "val_files = {f.replace(\".jpg\", \"\") for f in os.listdir(\"val-images\") if f.endswith(\".jpg\")}\n",
    "all_ids = list(train_files.union(val_files))\n",
    "\n",
    "df2 = data[data[\"image_id\"].isin(all_ids)].reset_index(drop=True)\n",
    "train_ids, val_ids = train_test_split(df2[\"image_id\"], test_size=0.2, random_state=42)\n",
    "train_df = df2[df2[\"image_id\"].isin(train_ids)].reset_index(drop=True)\n",
    "val_df = df2[df2[\"image_id\"].isin(val_ids)].reset_index(drop=True)\n",
    "print(\"Captions after filtering:\", len(df2))\n",
    "\n",
    "print(\"Train images:\", len(train_df))\n",
    "print(\"Val images:\", len(val_df))\n",
    "\n",
    "print(\"\\nSample train rows:\")\n",
    "print(train_df.head())\n",
    "\n",
    "print(\"\\nSample val rows:\")\n",
    "print(val_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5698639f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary built\n",
      "Vocabulary size: 2580\n",
      "\n",
      "First 20 tokens in vocab:\n",
      "['<pad>', '<unk>', '<start>', '<end>', 'the', 'a', 'in', 'and', 'there', 'is', 'see', 'can', 'on', 'we', 'are', 'this', 'of', 'image', 'i', 'background']\n"
     ]
    }
   ],
   "source": [
    "# build vocabulary\n",
    "SPECIALS = [\"<pad>\", \"<unk>\", \"<start>\", \"<end>\"]\n",
    "\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "def yield_tokens(captions):\n",
    "    for c in captions:\n",
    "        yield tokenize(c)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_df[\"caption\"]), specials=SPECIALS)\n",
    "vocab.set_default_index(vocab[\"<unk>\"])\n",
    "itos = vocab.get_itos()\n",
    "stoi = {w: i for i, w in enumerate(itos)}\n",
    "\n",
    "PAD_IDX = stoi[\"<pad>\"]\n",
    "BOS_IDX = stoi[\"<start>\"]\n",
    "EOS_IDX = stoi[\"<end>\"]\n",
    "\n",
    "print(\"Vocabulary built\")\n",
    "print(\"Vocabulary size:\", len(vocab))\n",
    "\n",
    "# show first 20 tokens\n",
    "print(\"\\nFirst 20 tokens in vocab:\")\n",
    "print(itos[:20])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "860ba878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset class\n",
    "class CaptioningDataset(Dataset):\n",
    "    def __init__(self, df, folder, stoi, size=(224, 224)):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.folder = folder\n",
    "        self.stoi = stoi\n",
    "        self.size = size\n",
    "        self.normalize = transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                              (0.229, 0.224, 0.225))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = f\"{self.folder}/{row.image_id}.jpg\"\n",
    "        if not os.path.exists(img_path):\n",
    "            return self.__getitem__(torch.randint(0, len(self.df), (1,)).item())\n",
    "\n",
    "        img = read_image(img_path)\n",
    "        if img.shape[0] == 1: img = img.repeat(3, 1, 1)\n",
    "        elif img.shape[0] == 4: img = img[:3]\n",
    "\n",
    "        img = TF.resize(img, self.size)\n",
    "        img = img.float() / 255.0\n",
    "        img = self.normalize(img)\n",
    "\n",
    "        tokens = row.caption.lower().split()\n",
    "        encoded = [self.stoi[\"<start>\"]] + [self.stoi.get(w, self.stoi[\"<unk>\"]) for w in tokens] + [self.stoi[\"<end>\"]]\n",
    "        return img, torch.tensor(encoded, dtype=torch.long), len(encoded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7f80c61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a method to work on batch data\n",
    "def collate_fn(batch):\n",
    "    batch.sort(key=lambda x: x[2], reverse=True)\n",
    "    imgs, caps, lens = zip(*batch)\n",
    "    imgs = torch.stack(imgs)\n",
    "    max_len = max(lens)\n",
    "    padded = torch.zeros(len(caps), max_len).long()\n",
    "    for i, cap in enumerate(caps):\n",
    "        padded[i, :len(cap)] = cap\n",
    "    return imgs, padded, torch.tensor(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8eab2497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 1672\n",
      "Val samples: 419\n"
     ]
    }
   ],
   "source": [
    "# create  datasets and dataloaders\n",
    "train_dataset = CaptioningDataset(train_df, \"train-images\", stoi)\n",
    "val_dataset = CaptioningDataset(val_df, \"val-images\", stoi)\n",
    "\n",
    "train_dl = DataLoader(train_dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "val_dl = DataLoader(val_dataset, batch_size=16, shuffle=False, collate_fn=collate_fn)\n",
    "print(\"Train samples:\", len(train_dataset))\n",
    "print(\"Val samples:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5863d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the CNN encoder\n",
    "class EncoderCNN(nn.Module):\n",
    "    def __init__(self, embed_dim=256):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)\n",
    "        self.cnn = nn.Sequential(*list(resnet.children())[:-1])  # remove final FC layer\n",
    "        for param in self.cnn.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.fc = nn.Linear(resnet.fc.in_features, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        feats = self.cnn(x).flatten(1)\n",
    "        return self.fc(feats)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7e856539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the RNN decoder\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=PAD_IDX)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.init_h = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.init_c = nn.Linear(embed_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, feats, inputs, lengths):\n",
    "        h = torch.tanh(self.init_h(feats)).unsqueeze(0)\n",
    "        c = torch.tanh(self.init_c(feats)).unsqueeze(0)\n",
    "        emb = self.embed(inputs)\n",
    "        packed = pack_padded_sequence(\n",
    "            emb, lengths.cpu(), batch_first=True, enforce_sorted=True\n",
    "        )\n",
    "        out, _ = self.lstm(packed, (h, c))\n",
    "        return self.fc(out.data)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def predict(self, feat, max_len=20):\n",
    "        # initialize hidden state from image features\n",
    "        h = torch.tanh(self.init_h(feat)).unsqueeze(0)\n",
    "        c = torch.tanh(self.init_c(feat)).unsqueeze(0)\n",
    "\n",
    "        # start with BOS token\n",
    "        word = torch.tensor([BOS_IDX], device=feat.device)\n",
    "        out_words = []\n",
    "\n",
    "        for _ in range(max_len):\n",
    "            emb = self.embed(word.unsqueeze(0))       # (1,1,embed_dim)\n",
    "            o, (h, c) = self.lstm(emb, (h, c))        # (1,1,hidden_dim)\n",
    "            logits = self.fc(o.squeeze(1))            # (1,vocab_size)\n",
    "\n",
    "            # choose next word\n",
    "            word = logits.argmax(1)                   # (1,)\n",
    "            tok = itos[word.item()]                   # map idx → token string\n",
    "\n",
    "            # skip repeating <start> tokens\n",
    "            if tok == \"<start>\":\n",
    "                continue\n",
    "\n",
    "            # stop at <end>\n",
    "            if tok == \"<end>\":\n",
    "                break\n",
    "\n",
    "            out_words.append(tok)\n",
    "\n",
    "        return \" \".join(out_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "571bf227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define training and validation steps\n",
    "def train_batch(imgs, caps, lens, encoder, decoder, optimizer, criterion):\n",
    "    encoder.train(); decoder.train()\n",
    "    feats = encoder(imgs)\n",
    "    out = decoder(feats, caps, lens)\n",
    "    tgt_packed = pack_padded_sequence(caps, lens.cpu(), batch_first=True).data\n",
    "    loss = criterion(out, tgt_packed)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_batch(imgs, caps, lens, encoder, decoder, criterion):\n",
    "    encoder.eval(); decoder.eval()\n",
    "    feats = encoder(imgs)\n",
    "    out = decoder(feats, caps, lens)\n",
    "    tgt_packed = pack_padded_sequence(caps, lens.cpu(), batch_first=True).data\n",
    "    loss = criterion(out, tgt_packed)\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bb8be909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b43f280930044b2911f0ec57aa47532",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1 - Training:   0%|          | 0/105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m imgs, caps, lens \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dl, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Training\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     16\u001b[39m     imgs, caps, lens = imgs.to(device), caps.to(device), lens.to(device)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     loss = \u001b[43mtrain_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     train_losses.append(loss)\n\u001b[32m     20\u001b[39m enc.eval(); dec.eval()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtrain_batch\u001b[39m\u001b[34m(imgs, caps, lens, encoder, decoder, optimizer, criterion)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain_batch\u001b[39m(imgs, caps, lens, encoder, decoder, optimizer, criterion):\n\u001b[32m      3\u001b[39m     encoder.train(); decoder.train()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     feats = \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     out = decoder(feats, caps, lens)\n\u001b[32m      6\u001b[39m     tgt_packed = pack_padded_sequence(caps, lens.cpu(), batch_first=\u001b[38;5;28;01mTrue\u001b[39;00m).data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mEncoderCNN.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     feats = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcnn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m.flatten(\u001b[32m1\u001b[39m)\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.fc(feats)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torchvision/models/resnet.py:154\u001b[39m, in \u001b[36mBottleneck.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    151\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn2(out)\n\u001b[32m    152\u001b[39m out = \u001b[38;5;28mself\u001b[39m.relu(out)\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    155\u001b[39m out = \u001b[38;5;28mself\u001b[39m.bn3(out)\n\u001b[32m    157\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.downsample \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/nlp_env/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(F.pad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode),\n\u001b[32m    457\u001b[39m                     weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    458\u001b[39m                     _pair(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "enc = EncoderCNN(256).to(device)\n",
    "dec = DecoderRNN(256, 512, len(vocab)).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(dec.parameters(), lr=1e-3)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "\n",
    "EPOCHS = 5\n",
    "log = Report(EPOCHS)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    enc.train(); dec.train()\n",
    "    train_losses = []\n",
    "\n",
    "    for imgs, caps, lens in tqdm(train_dl, desc=f\"Epoch {epoch+1} - Training\"):\n",
    "        imgs, caps, lens = imgs.to(device), caps.to(device), lens.to(device)\n",
    "        loss = train_batch(imgs, caps, lens, enc, dec, optimizer, criterion)\n",
    "        train_losses.append(loss)\n",
    "\n",
    "    enc.eval(); dec.eval()\n",
    "    val_losses = []\n",
    "\n",
    "    for imgs, caps, lens in tqdm(val_dl, desc=f\"Epoch {epoch+1} - Validation\"):\n",
    "        imgs, caps, lens = imgs.to(device), caps.to(device), lens.to(device)\n",
    "        val_loss = validate_batch(imgs, caps, lens, enc, dec, criterion)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "    # record average losses for the epoch\n",
    "    log.record(epoch + 1, trn_loss=np.mean(train_losses), val_loss=np.mean(val_losses))\n",
    "    log.report_avgs(epoch + 1)\n",
    "\n",
    "# plot after all epochs\n",
    "log.plot_epochs(log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf34eb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction function\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# same normalization used in your dataset\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ConvertImageDtype(torch.float32),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                         (0.229, 0.224, 0.225))\n",
    "])\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_caption(image_path):\n",
    "    img = read_image(image_path)\n",
    "\n",
    "    # fix grayscale / RGBA\n",
    "    if img.shape[0] == 1:\n",
    "        img = img.repeat(3, 1, 1)\n",
    "    elif img.shape[0] == 4:\n",
    "        img = img[:3]\n",
    "\n",
    "    img = img_transform(img)\n",
    "    img = img.unsqueeze(0).to(device) # add batch dimension\n",
    "\n",
    "    enc.eval()\n",
    "    dec.eval()\n",
    "\n",
    "    # encode image → get embedding\n",
    "    feats = enc(img)\n",
    "\n",
    "    # call decoder to generate caption\n",
    "    caption = dec.predict(feats)\n",
    "\n",
    "    return caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d82a62cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models for prediction\n",
    "enc = EncoderCNN(256)\n",
    "dec = DecoderRNN(256, 512, len(vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7561f161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: val-images/7773dcec1af6dd14.jpg\n",
      "Caption: gate,wall rays. singing. lake plant, signboard, ocean an costume carpets, surface. cloud. fences, jacket. crane gun, moving. full autos. brown,grey\n"
     ]
    }
   ],
   "source": [
    "# load trained model weights\n",
    "\n",
    "test_img = random.choice(os.listdir(\"val-images\"))\n",
    "test_path = f\"val-images/{test_img}\"\n",
    "\n",
    "print(\"Image:\", test_path)\n",
    "print(\"Caption:\", predict_caption(test_path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80799b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
